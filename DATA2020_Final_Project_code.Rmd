---
title: "DATA2020 final project(Feature Selection)"
author: "Mingjun Ma"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(modelr)
library(caret)
library(Hmisc)
library(readstata13)
library(data.table)
library(readr)
library(haven)
library(Metrics)
library(randomForest)
library(tidymodels)
library(themis)
library(sjstats)
library(pscl)
library(glmnet)
library(car)
library(ROSE)
library(reshape2)
```


```{r}
df <- read_dta("FF_wave5_2020v2.dta")
```


```{r}
target <- c("k5e2a", "k5e2b", "k5e2c", "k5e2d")
independent_features <- c("k5b1d", "k5b3c", "k5c2", "k5c3", "k5f1b", "k5f1e",
                          "k5g2b", "k5g2e", "k5g2f","k5g2g", "k5g2h", "k5g2k", 
                          "k5g2l", "k5g2n", "m5b3", "f5a4j1", "f5b16d", 
                          "f5f23h", "n5f2b1g", "n5f10", "p5q3ak", "p5q3bj", 
                          "ch5dspr", "ch5wj10pr", "o5a7a", "o5a9", "o5d3", 
                          "o5d4", "o5e8")
variables_list <- setdiff(names(df), target)
df <- df %>% 
  filter(!if_any(all_of(target), ~ . < 0))
```


```{r}
df <- df %>%
  filter(!if_any(all_of(target), ~ . < 0)) %>%
  select(all_of(c(target, variables_list)))
```


```{r}
# Prepare the dataset
df <- df %>%
  select(all_of(c(target, variables_list))) %>%
  mutate(across(all_of(target), ~as.numeric(as.character(.))))
```


```{r}
# Summarize targets into a single response variable
y <- rowSums(select(df, all_of(target)))

df <- df %>%
  select(-all_of(target))
```


```{r}
# List of invalid responses
invalid_responses <- c(-1, -2, -3,
                       -5, -6, -7,
                       -8, -9, -10, -12, -14)
```


```{r}
# Replace invalid responses with "-1" and convert to numeric
df <- df %>%
  mutate(across(everything(), ~as.character(.))) %>%  # Ensure everything is character
  mutate(across(everything(), ~ifelse(. %in% invalid_responses, "-1", .))) %>%
  mutate(across(everything(), as.numeric))
```

```{r}
# Convert all character columns to factors (this assumes 'df' is currently a correct dataframe)
df <- df %>%
  mutate(across(where(is.character), as.factor))

# Impute missing data with median values for numeric columns
df <- df %>%
  mutate(across(where(is.numeric), 
                ~ifelse(is.na(.), 
                median(., na.rm = TRUE), .)))

# Create dummy variables using 'dummyVars'
dv <- dummyVars(~., data = df)
df_transformed <- predict(dv, newdata = df)

# Convert the output matrix to a dataframe
df_transformed <- as.data.frame(df_transformed)

# Apply standardization to the numeric columns of the transformed dataframe
df_scaled <- scale(df_transformed)
```


```{r}
# Ensure y is binary and split data
y_binary <- ifelse(y == 0, 0, 1)

set.seed(42)  # Ensure reproducibility
train_index <- createDataPartition(y_binary, p = 0.8, list = FALSE)
y_train_binary <- y_binary[train_index]
y_test_binary <- y_binary[-train_index]
X_train <- df_scaled[train_index, ]
X_test <- df_scaled[-train_index, ]
```

```{r}
##EDA Part 


##Part-1 Target Variables(y to y_binary)
##How to calculate y? : From the User Guidebook, we decide to take sum of four variables which are highly correlated with peer bully to define the bullying degree. 

# k5e2a Picked on you or said mean things to you?  k5e2b Hit you?  Taken your things, like your money or lunch, without asking? k5e2c.   Purposely left you out of activities?  k5e2d


#From the plort, we can observe existing imbalance, thus we decide to treat the sum whose value is smaller than 0 to "label 0" (never being bullied before) and sum whose value is larger than 0 as label 1(being bullied before). Now, the distribution of target variable is balanced. 
library(gridExtra)

# Creating the histogram for y
plot_y <- ggplot(data.frame(y), aes(x=y)) +
  geom_histogram(bins=15, fill="blue", alpha=0.7) +
  labs(title="Distribution of y", x="Value", y="Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Creating the histogram for y_binary
#plot_y_binary <- ggplot(data.frame(y_binary), aes(x=y_binary)) +
#  geom_histogram(bins=15, fill="blue", alpha=0.7) +
#  labs(title="Distribution of y_binary", x="Value", y="Frequency") +
#  theme_minimal() +
#  theme(plot.title = element_text(hjust = 0.5))

plot_y_binary <- ggplot(data.frame(y_binary), aes(x=factor(y_binary))) +
  geom_bar(fill="blue", alpha=0.7, width = 0.3) +  # Reduced width for a narrower bar
  labs(title="Distribution of y_binary", x="Value", y="Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))


# Arrange the plots side by side
grid.arrange(plot_y, plot_y_binary, ncol = 2)
```


```{r}
##EDA Part-2 
##Reduced Features 
###In the original dataset, there are so many features(around 3864 features). To reduce the number of features, first we choose alpha=0.2 to achieve the largest R^2 and based on this model, we select 30 features whose coefficient does not equal to 0. 

library(glmnet)

# Identify columns with any missing values
cols_with_na <- apply(X_train, 2, function(x) any(is.na(x)))

# Subset the DataFrame to keep only columns without missing values
X_train_no_na <- X_train[, !cols_with_na]

# Now convert to a matrix if necessary
X_train_matrix <- as.matrix(X_train_no_na)

# Don't forget to adjust the feature names stored if needed
feature_names <- colnames(X_train_no_na)

X_test_matrix <- as.matrix(X_test)
lasso_model <- glmnet(X_train_matrix, y_train_binary, family = "binomial", alpha = 1)

cv_lasso <- cv.glmnet(X_train_matrix, y_train_binary, family = "binomial", alpha = 1)

# Extract coefficients at the lambda that minimizes the cross-validation error
best_lambda <- cv_lasso$lambda.min
coefficients_matrix <- coef(cv_lasso, s = 'lambda.min')

# Convert coefficients to a regular numeric vector, dropping the intercept
coefficients_vector <- as.numeric(coefficients_matrix[-1, 1])

# Get names of the variables (features) with non-zero coefficients
non_zero_coefficients <- feature_names[coefficients_vector != 0]

# Store these variables in a list
selected_features <- list(variables = non_zero_coefficients)

# You can print or return this list to see which features were selected
print(selected_features)




##EDA-PART_2 Missing Values 

#(1) We drop features with NAN values(those features are not important themselves)
#(2) For answers like "-1 Refuse", "-2 Don\'t know", "-3 Missing",..etc, we replace those values with -1. 




##EDA-PART_4_Model Assumption (Logistic Regression )


#Co-linearlity betwen features (VIF)

logistic_model <- glm(y_binary ~ ., 
                      data = data.frame(df, y_binary), 
                      family = "binomial")


vif_values <- vif(logistic_model)

vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values)

vif_plot <- ggplot(vif_df, aes(x = reorder(Variable, -VIF), y = VIF)) +  # Note the minus sign to sort descending
  geom_col(fill = "steelblue") +
  labs(title = "VIF of Features", x = "Features", y = "Variance Inflation Factor") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),  # Rotate x labels for better visibility
        plot.title = element_text(hjust = 0.5))  # Center the title

# Print the plot
print(vif_plot)


## PCA 


pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE, rank. = 2)


pc_scores <- pca_result$x

pca_df <- data.frame(PC1 = pc_scores[, 1], PC2 = pc_scores[, 2], Target = y_binary)

pca_plot <- ggplot(pca_df, aes(x = PC1, y = PC2, color = as.factor(Target))) +
  geom_point(alpha = 0.5) +
  labs(title = "PCA Plot of the First Two Principal Components",
       x = "Principal Component 1",
       y = "Principal Component 2",
       color = "Target Variable") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))  # Center the title

# Print the plot
print(pca_plot)


```




```{r}

##Model And Results 

#Logistic Regression

y_train_binary <- as.factor(y_train_binary)

# Random Forest Model for Classification
rf_classifier <- randomForest(
  x = X_train,    
  y = y_train_binary, 
  ntree = 150,       
  mtry = sqrt(ncol(X_train)),
  nodesize = 5,          
  maxnodes = 15,  
  sampsize = floor(0.632*nrow(X_train)), 
  importance = TRUE,
  randomForest = 42
)

# Logistic Classification Model
logistic_model <- glm(y_train_binary ~ ., 
                      data = data.frame(X_train, y_train_binary), 
                      family = "binomial")

logistic_probs <- predict(logistic_model, 
                          newdata = data.frame(X_test), 
                          type = "response")

logistic_predictions <- ifelse(logistic_probs > 0.5, 1, 0)




rf_predictions <- predict(rf_classifier, newdata = X_test, type = "response")
rf_accuracy <- mean(rf_predictions == y_test_binary, na.rm = TRUE)
logistic_accuracy <- mean(logistic_predictions == y_test_binary, na.rm = TRUE)

# Print accuracy values
print(paste("Optimized Random Forest Accuracy:", rf_accuracy))
print(paste("Logistic Regression Accuracy:", logistic_accuracy))

# Ensure both are factors with the same levels
y_test_binary <- factor(y_test_binary, levels = c("0", "1"))
logistic_predictions <- factor(logistic_predictions, levels = c("0", "1"))

# Compute the confusion matrix
conf_matrix <- confusionMatrix(logistic_predictions, y_test_binary)

# Convert the confusion matrix to a table and then to a dataframe for plotting
matrix_df <- as.data.frame(as.table(conf_matrix$table))

# Reshape for better visualization
names(matrix_df) <- c("Reference", "Prediction", "Frequency")

# Creating the plot
ggplot(matrix_df, aes(x = Reference, y = Prediction, fill = Frequency)) +
  geom_tile(color = "white") +  # Use geom_tile() for creating tiles
  geom_text(aes(label = Frequency), vjust = 1.5, color = "black", size = 5) +  # Add frequencies text
  scale_fill_gradient(low = "white", high = "steelblue") +  # Gradient color fill
  labs(title = "Confusion Matrix (Logistic)", x = "Actual Class", y = "Predicted Class") +
  theme_minimal() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
      

# Ensure that predictions and actual values are factors with the same levels
y_test_binary <- factor(y_test_binary, levels = c("0", "1"))
rf_predictions <- factor(rf_predictions, levels = c("0", "1"))

# Compute the confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, y_test_binary)

# Convert the confusion matrix to a dataframe
matrix_df_rf <- as.data.frame(as.table(conf_matrix_rf$table))
names(matrix_df_rf) <- c("Reference", "Prediction", "Frequency")

# Create the plot
ggplot(matrix_df_rf, aes(x = Reference, y = Prediction, fill = Frequency)) +
  geom_tile(color = "white") +  # Creates tiles
  geom_text(aes(label = Frequency), vjust = 1.5, color = "black", size = 5) +  # Adds text to tiles
  scale_fill_gradient(low = "white", high = "steelblue") +  # Adds gradient color
  labs(title = "Confusion Matrix for Random Forest", x = "Actual Class", y = "Predicted Class") +
  theme_minimal() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
        
# Ensure that predictions and actual values are factors with the same levels
y_test_binary <- factor(y_test_binary, levels = c("0", "1"))
rf_predictions <- factor(rf_predictions, levels = c("0", "1"))

# Compute the confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, y_test_binary)

# Convert the confusion matrix to a dataframe
matrix_df_rf <- as.data.frame(as.table(conf_matrix_rf$table))
names(matrix_df_rf) <- c("Reference", "Prediction", "Frequency")

# Create the plot
ggplot(matrix_df_rf, aes(x = Reference, y = Prediction, fill = Frequency)) +
  geom_tile(color = "white") +  # Creates tiles
  geom_text(aes(label = Frequency), vjust = 1.5, color = "black", size = 5) +  # Adds text to tiles
  scale_fill_gradient(low = "white", high = "steelblue") +  # Adds gradient color
  labs(title = "Confusion Matrix for Random Forest", x = "Actual Class", y = "Predicted Class") +
  theme_minimal() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
```

```{r}
# Extract importance scores
importance_scores <- importance(rf_classifier)

# Convert to data frame for easier handling
importance_df <- as.data.frame(importance_scores)

# Add row names as a variable for tracking variables names
importance_df$Variable <- rownames(importance_df)

# Depending on your randomForest setup, you might have different measures (e.g., MeanDecreaseAccuracy, MeanDecreaseGini)
# Adjust the column name accordingly if different
top_importances <- importance_df %>%
  dplyr::arrange(desc(MeanDecreaseGini)) %>%  # Sort by importance measure, descending
  dplyr::slice(1:10)  # Select top 10

# Display the top 10 important features
print(top_importances)

ggplot(top_importances, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini, fill = MeanDecreaseGini)) +
  geom_col() +
  coord_flip() +  # Horizontal bar plot for better visibility of variable names
  labs(x = "Variables", y = "Importance", 
       title = "Top 10 Variable Importances in Random Forest") +
  theme_minimal()

library(caret)
library(randomForest)

# Setting up the training control
control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  search="grid"
)

tunegrid <- expand.grid(.mtry=c(sqrt(ncol(X_train))))
modellist <- list()
seed <- 42
metric <- "Accuracy"

for (ntree in c(100, 150, 200)) {
  set.seed(seed)
  fit <- train(
    x = X_train,
    y = y_train_binary,
    method = "rf",
    trControl = control,
    tuneGrid = tunegrid,
    metric = metric
  )
  key <- toString(ntree)
  modellist[[key]] <- fit
}

results <- resamples(modellist)
summary(results)
dotplot(results)


# Ensure y_train_binary is a factor and has valid R variable names
y_train_binary <- factor(y_train_binary)
levels(y_train_binary) <- make.names(levels(y_train_binary))

folds <- createFolds(y_train_binary, k = 5, list = TRUE, returnTrain = FALSE)

accuracy_vector <- numeric(length(folds))

for (i in seq_along(folds)) {
  test_indices <- folds[[i]]
  train_indices <- setdiff(seq_len(nrow(X_train)), test_indices)

  rf_classifier <- randomForest(
    x = X_train[train_indices, ],    
    y = y_train_binary[train_indices], 
    ntree = 150,       
    mtry = floor(sqrt(ncol(X_train))),
    nodesize = 5,          
    maxnodes = 15,  
    sampsize = floor(0.632 * length(train_indices)), 
    importance = TRUE,
    set.seed = 42 
  )
  
  predictions <- predict(rf_classifier, X_train[test_indices, ])

  accuracy_vector[i] <- sum(predictions == y_train_binary[test_indices]) / length(test_indices)
}

for (i in 1:5) {
  print(paste("Accuracy fold", i, ":", accuracy_vector[i]))
}
print(paste("Mean Accuracy Over 5 folds:", mean(accuracy_vector)))
```



